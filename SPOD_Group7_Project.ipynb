{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Weather in Australia - Team 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This cell just loads all used moduls for running the notebook. Please install any package if you dont have it installed in your environment so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#disable some annoying warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "#----------------------------#\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "#plots the figures in place instead of a new window\n",
    "%matplotlib inline\n",
    "\n",
    "import statistics\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import decomposition\n",
    "from numpy import unique\n",
    "from numpy import where\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pyplot\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Dataset Overview\n",
    "\n",
    "We chose the rain in Australia dataset from Kaggle because we thought that it could be interesting to analyze a dataset with around 145000 rows. It is also interesting that data from about 10 years of daily observations from different locations throughout Australia has been collected.\n",
    "\n",
    "Besides several numerical attributes, also several categorical attributes are provided. The attributes of the used dataset are explained below.\n",
    "\n",
    "<ol>\n",
    "    <li> Date: The observation's date\n",
    "    <li> Location: The location of the observation\n",
    "    <li> MinTemp: The minimum temperature on that day (째C)\n",
    "    <li> MaxTemp: The maximum temperature on that day (째C)\n",
    "    <li> Rainfall: The rainfall amount measured in mm\n",
    "    <li> Evaporation: The evaporation also measured in mm\n",
    "    <li> Sunshine: The number of sunshine hours\n",
    "    <li> WindGustDir: The strongest wind gust's direction\n",
    "    <li> WindGustSpeed: The strongest wind gust's speed in km/h\n",
    "    <li> WindDir9am: The wind's direction at 9 AM\n",
    "    <li> WindDir3pm: The wind's direction at 3 PM\n",
    "    <li> WindSpeed9am: The wind's speed (km/h) at 9 AM\n",
    "    <li> WindSpeed3pm: The wind's speed (km/h) at 3 PM\n",
    "    <li> Humidity9am: The humidity percentage at 9 AM\n",
    "    <li> Humidity3pm: The humidity percentage at 3 PM\n",
    "    <li> Pressure9am: The atmospheric pressure (hpa) at 9 AM\n",
    "    <li> Pressure3pm: The atmospheric pressure (hpa) at 3 PM\n",
    "    <li> Cloud9am: Fraction of obscured sky by clouds (in \"oktas\") at 9 AM\n",
    "    <li> Cloud3pm: Same as above but at 3 PM\n",
    "    <li> Temp9am: Temperature in 째C at 9 AM\n",
    "    <li> Temp3pm: Temperature in 째C at 3 PM\n",
    "    <li> RainToday: True, if it has been raining on that day, otherwise False\n",
    "    <li> RainTomorrow: True, if it has been raining on the next day, otherwise False; target variable\n",
    "<ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# use the weather dataset of heterogenous data and plot first 5 lines\n",
    "weather = pd.read_csv('data/weatherAUS.csv')\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# overview of the created datatypes\n",
    "weather.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Preparation - Adjust Date Values\n",
    "\n",
    "In this step, the data gets adjusted, in order to fit for our analysis.\n",
    "This adjustments go especially for the Date in the first place. Here the whole Date value gets split up into a new year month and day column, in order to better aggregate over the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Convert Date to a date type and create new columns\n",
    "weather['Date_converted'] = pd.to_datetime(weather['Date'], format='%Y-%m-%d')\n",
    "weather['Year'] = weather['Date_converted'].dt.year\n",
    "weather['Month'] = weather['Date_converted'].dt.month\n",
    "weather['Day'] = weather['Date_converted'].dt.day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Overview of missing values\n",
    "\n",
    "In order to to a proper data cleaning and having a feeling, how many values are even missing, we analysed the amount of missing data per column. It can be seen that for some columns nearly half of the values (40 - 48%) are missing (shown in the table as well as the plot above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate percentage of null values per attribute\n",
    "missing_in_percentage = weather.isnull().sum() * 100 / len(weather)\n",
    "missing = pd.DataFrame({'col': weather.columns, 'missing_percent': missing_in_percentage})\n",
    "missing.sort_values('missing_percent', inplace=True, ascending=False)\n",
    "\n",
    "ax = sns.barplot(x=\"col\", y=\"missing_percent\", data=missing.head(8))\n",
    "ax.set_ylim((0, 100))\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=300)\n",
    "ax.set_title('Percentage of missing values per DF column')\n",
    "ax.set_xlabel('DF columns')\n",
    "_ = ax.set_ylabel('Percentage of missing values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Base for missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Missing values in different seasons\n",
    "\n",
    "Now we further investigate this issue by looking at the columns sunshine, evaporation, cloud3pm and cloud9am by grouping the percentage of missing values first by season, to look whether we can see a seasonal affect. We also group the percentage of missing values by location to see if we can spot a locational affect. But as you can also see in the table below, there is no real trend, if the values tend to be not recorded in a specific season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Mapping the dates to seasons and calculate for each season and attribute the percentage of missing values.\n",
    "seasons = {\n",
    "   1: 'Winter',\n",
    "   2: 'Spring',\n",
    "   3: 'Summer',\n",
    "   4: 'Autumn'\n",
    "}\n",
    "df_values_season = weather[['Year', 'Month', 'Sunshine', 'Evaporation', 'Cloud3pm', 'Cloud9am']].copy()\n",
    "\n",
    "df_values_season['Season'] = (df_values_season['Month'] % 12 + 3) // 3\n",
    "df_values_season['Season_name'] = df_values_season['Season'].map(seasons)\n",
    "\n",
    "df_season_count_null = df_values_season[['Sunshine', 'Evaporation', 'Cloud3pm', 'Cloud9am']].isnull().groupby(df_values_season['Season_name']).sum()\n",
    "df_season_count_all = df_values_season[['Sunshine', 'Evaporation', 'Cloud3pm', 'Cloud9am']].isnull().groupby(df_values_season['Season_name']).count()\n",
    "\n",
    "df_missing_values_percent = (df_season_count_null / df_season_count_all) * 100\n",
    "df_missing_values_percent['Season'] = df_missing_values_percent.index.tolist()\n",
    "df_missing_values_percent.style.hide_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Missing values in different locations\n",
    "\n",
    "As it can be seen, for 22 of the 49 locations no values are tracked which explains the large amount of missing data for the attributes 'Sunshine', 'Evaporation', 'Cloud3pm' and 'Cloud9am'. The reason for this is, however, unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_values_location = weather[['Location', 'Sunshine', 'Evaporation', 'Cloud3pm', 'Cloud9am']]\n",
    "df_values_location_count_null = weather[['Sunshine', 'Evaporation', 'Cloud3pm', 'Cloud9am']].isnull().groupby(weather['Location']).sum()\n",
    "# fillna is needed in order to get the \n",
    "df_values_location_count_all = weather[['Sunshine', 'Evaporation', 'Cloud3pm', 'Cloud9am']].isnull().groupby(weather['Location']).count()\n",
    "\n",
    "df_missing_values_percent = (df_values_location_count_null / df_values_location_count_all) * 100\n",
    "df_missing_values_percent['Location'] = df_missing_values_percent.index.tolist()\n",
    "mask = (df_missing_values_percent == 100.).any(axis=1)\n",
    "print(f'Untracked values based on location: {df_missing_values_percent[mask].shape[0]} of {df_missing_values_percent.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Remove missing values\n",
    "\n",
    "Since we can not clearly 'clean' missing values in any case, because we dont have information about the geo coordinates and also no mapping of close location, we simply drop these values. Still - 112925 samples are present "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "weather.drop(['Date','Sunshine', 'Evaporation', 'Cloud3pm', 'Cloud9am'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create artifical data for missing values in numeric attribute vectors when possible\n",
    "\n",
    "For numeric data we set missing values for numeric attributes (given in the numerical_columns value) to the median based on the year, month and (location) when possible\n",
    "\n",
    "For the categorical values we used the mode, imputation is based on location and current month, if we do not have data for a location than only the month was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "numerical_columns = [\"Pressure9am\", \"Pressure3pm\", \"Humidity3pm\", \"Humidity9am\", \"WindGustSpeed\", \"Temp3pm\",\n",
    "                     \"WindSpeed3pm\", \"WindSpeed9am\", \"Temp9am\", \"MinTemp\", \"MaxTemp\", \"Rainfall\"]\n",
    "\n",
    "for col in numerical_columns:\n",
    "    weather[col] = weather[col].fillna(weather.groupby(['Year', 'Month', 'Location'])[col].transform(\"mean\"))\n",
    "    weather[col] = weather[col].fillna(weather.groupby(['Year', 'Month'])[col].transform(\"mean\"))\n",
    "\n",
    "categorical_columns = [\"WindDir9am\", \"WindGustDir\", \"WindDir3pm\"]\n",
    "\n",
    "for col in categorical_columns:\n",
    "    weather[col] = weather[col].fillna(weather.groupby(['Year', 'Month', 'Location'])[col].transform(statistics.mode))\n",
    "    weather[col] = weather[col].fillna(weather.groupby(['Year', 'Month'])[col].transform(statistics.mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "weather.dropna(inplace=True)\n",
    "print(f'Amount of samples without missing values in any column: {weather.shape[0]}')\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Check for valid values in all remaining (numeric) columns\n",
    "\n",
    "In the next step, # check for minimum and maximum values in numeric attributes (in our case all attributes in the frame which have the datatype of float64. Here no out of range values could be detected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# check for minimum and maximum values in numeric attributes:\n",
    "for col in weather.loc[:, weather.dtypes == 'float64']:\n",
    "    print(f'Attribute {col}:')\n",
    "    print(\"Min: {:.2f}, Q1: {:.2f}, Median {:.2f}, Q3: {:.2f}, Max: {:.2f}\".format(weather[col].min(),weather[col].quantile(.25),weather[col].median(),weather[col].quantile(.75), weather[col].max()))\n",
    "    sns.boxplot(x=weather[col])\n",
    "    plt.title(f'Boxplot of Attribute {col}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Check the distribution of RainTomorrow samples\n",
    "\n",
    "As we can clearly see in the next cell, there are a lot more samples of NOT-raining tomorrow, as samples WITH raining tomorrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.countplot(x=\"RainTomorrow\", data=weather);\n",
    "plt.title('Original Data distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Disproportionate sampling:\n",
    "# randomly select 4 samples from each stratum\n",
    "stratified = weather.groupby('RainTomorrow', group_keys=False).apply(lambda x: x.sample(31000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.countplot(x=\"RainTomorrow\", data=stratified)\n",
    "plt.title(\"Stratified Data (equal bins)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# PCA to explore the underlying structure of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stratified.drop('Date_converted',axis=1,inplace=True)\n",
    "for col in stratified.loc[:, stratified.dtypes == object]:\n",
    "    # creating instance of labelencoder\n",
    "    labelencoder = LabelEncoder()\n",
    "    # Assigning numerical values and storing in another column\n",
    "    stratified[f'{col}_num'] = labelencoder.fit_transform(stratified[col])\n",
    "    # drop non-numeric column\n",
    "    stratified.drop(col,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stratified.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_components = 7\n",
    "\n",
    "pca = decomposition.PCA(n_components=n_components)\n",
    "pca_pos = pca.fit_transform(stratified)\n",
    "\n",
    "stratified['pca1']= pca_pos[:, 0]\n",
    "stratified['pca2']= pca_pos[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "reducedPoints = stratified.groupby('RainTomorrow_num', group_keys=False).apply(lambda x: x.sample(2500))\n",
    "sns.scatterplot(data=reducedPoints, x=\"pca1\", y=\"pca2\", hue=\"RainTomorrow_num\",alpha=0.3)\n",
    "plt.title('PCA on the weather dataset, colored by RainTomorrow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Decision Tree\n",
    "\n",
    "In this section, we try to fit a Decision Tree classifier to our data. Therefore we do a GridSearch, where we try different criterions, maximum depths of the tree and splitting methods. The trained classifier also gets evaluated on 15% of the total data afterwards.\n",
    "\n",
    "To keep the dataset clean, we removed all additional added attributes, we used in the previous section due to have more comfort. This does not change the actual data at all.\n",
    "\n",
    "Note, that the data is also stratified like in the PCA above, so all classes are evenly distributed (standard would be to have a much higher amount of samples in the RainTomorrow=No comapred to RainTomorrow=Yes)\n",
    "\n",
    "After creating the training and test sets, training and evaluating using a confusion matrix and accuracy as a score, we also provided an overview of the feature importance learned by the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluates the model and returns accuracy as well as a confusion matrix. Also the time for prediction can is calculated.\n",
    "@param model, sklearn model,trained model\n",
    "@param x_test, np ndarray, data matrix\n",
    "@param y_test, np ndarray, data vector\n",
    "\"\"\"\n",
    "def get_evaluation(model, x_test, y_test):\n",
    "    y_pred = model.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    rec_result = recall_score(y_test, y_pred, average=None, labels=[0,1])\n",
    "    prec_result = precision_score(y_test, y_pred, average=None, labels=[0,1])\n",
    "    \n",
    "\n",
    "    print('\\nAccuracy of Classifier on Test Image Data: ', accuracy)\n",
    "    print()\n",
    "    print('Recall (No Rain Tomorrow) of Classifier on Test Image Data: ', rec_result[0])\n",
    "    print('Recall (Rain Tomorrow) of Classifier on Test Image Data: ', rec_result[1])\n",
    "    print()\n",
    "    print('Precision (No Rain Tomorrow) of Classifier on Test Image Data: ', prec_result[0])\n",
    "    print('Precision (Rain Tomorrow) of Classifier on Test Image Data: ', prec_result[1])\n",
    "    print()\n",
    "    print('\\nConfusion Matrix: \\n', conf_mat)\n",
    "\n",
    "    plt.matshow(conf_mat)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'criterion': ['gini','entropy'],\n",
    "    'max_depth': range(1,20),\n",
    "    'splitter': ['random', 'best']\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "Trains a random forest using cross-validation and returns certain attributes of the received model including the best\n",
    "parameter combination.\n",
    "@param x_train, np ndarray, data matrix\n",
    "@param y_train, np ndarray, data vector\n",
    "@param param_grid, dict, grid holding the paramaters for search\n",
    "\"\"\"\n",
    "def train_dec_tree(x_train,y_train,param_grid):\n",
    "    tree = DecisionTreeClassifier(random_state=55)\n",
    "    model = GridSearchCV(tree,param_grid=param_grid,n_jobs = -1)\n",
    "    model.fit(x_train,y_train)\n",
    "    return model.best_params_,model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# remove target value and addtional added columns\n",
    "X = stratified.drop(['RainTomorrow_num','pca1','pca2'], axis=1)\n",
    "y = stratified['RainTomorrow_num']\n",
    "print(f'shape of data matrix: {X.shape}')\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "print(f'shape of train matrix: {x_train.shape}')\n",
    "print(f'shape of test matrix: {x_test.shape}')\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# train decision tree with created training set and evaluate on created target set\n",
    "params_dec_tree, model_dec_tree = train_dec_tree(x_train, y_train, param_grid)\n",
    "_ = get_evaluation(model_dec_tree, x_test, y_test)\n",
    "print(\"The best parameters are: {}\".format(params_dec_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create overview of feature importance, of learned decision tree\n",
    "attribute_weights = pd.DataFrame({\n",
    "    'Attribute' : x_train.columns,\n",
    "    'Weight' : model_dec_tree.feature_importances_\n",
    "}).sort_values(by='Weight', ascending=False)\n",
    "plt.title('Importance of different Attributes')\n",
    "sns.barplot(data = attribute_weights, x='Weight', y='Attribute');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "param_grid_forest = {\n",
    "    'criterion': ['gini','entropy'],\n",
    "    'max_depth': range(5,25)\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "Trains a random forest using cross-validation and returns certain attributes of the received model including the best\n",
    "parameter combination.\n",
    "@param x_train, np ndarray, data matrix\n",
    "@param y_train, np ndarray, data vector\n",
    "@param param_grid, dict, grid holding the paramaters for search\n",
    "\"\"\"\n",
    "def train_random_forest(x_train,y_train,param_grid):\n",
    "    ensemble = RandomForestClassifier(random_state=55)\n",
    "    model = GridSearchCV(ensemble,param_grid=param_grid, n_jobs = -1)\n",
    "    model.fit(x_train,y_train)\n",
    "    return model.best_params_,model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# train decision tree with created training set and evaluate on created target set\n",
    "params_random_forest, model_random_forest = train_random_forest(x_train, y_train, param_grid_forest)\n",
    "_ = get_evaluation(model_random_forest, x_test, y_test)\n",
    "print(\"The best parameters are: {}\".format(params_random_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create overview of feature importance, of learned decision tree\n",
    "attribute_weights = pd.DataFrame({\n",
    "    'Attribute' : x_train.columns,\n",
    "    'Weight' : model_random_forest.feature_importances_\n",
    "}).sort_values(by='Weight', ascending=False)\n",
    "plt.title('Importance of different Attributes')\n",
    "sns.barplot(data = attribute_weights, x='Weight', y='Attribute');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_ = get_evaluation(xgb, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create overview of feature importance, of learned decision tree\n",
    "attribute_weights = pd.DataFrame({\n",
    "    'Attribute' : x_train.columns,\n",
    "    'Weight' : xgb.feature_importances_\n",
    "}).sort_values(by='Weight', ascending=False)\n",
    "plt.title('Importance of different Attributes')\n",
    "sns.barplot(data = attribute_weights, x='Weight', y='Attribute');"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Regression\n",
    "\n",
    "In this part, are going to develop an estimator for the rainfall. Since, rainfall is a continuous variable, this is obviously a regression task.\n",
    "\n",
    "Since, this is going to be a multiple regression task, and therefore, not all variables might have a significant impact, we chose the best subset selection method\n",
    "for identifying the required variables."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data preparation for the regression part"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_train_reg = x_train.loc[:, x_train.columns != 'Rainfall'].copy()\n",
    "x_test_reg = x_test.loc[:, x_test.columns != 'Rainfall'].copy()\n",
    "\n",
    "y_train_reg = x_train['Rainfall'].copy()\n",
    "y_test_reg = x_test['Rainfall'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, after the data is prepared for the regression part, we now start the subset selection to retrieve"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}